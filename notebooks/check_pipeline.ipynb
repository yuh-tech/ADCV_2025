{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd3a05e",
   "metadata": {},
   "source": [
    "# üß™ Dataset Pipeline Testing\n",
    "\n",
    "This notebook tests the data loading pipeline for both EuroSAT and BigEarthNet datasets.\n",
    "\n",
    "**Works on:**\n",
    "- ‚úÖ Local environment\n",
    "- ‚úÖ Kaggle environment (auto-detects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6396a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SETUP AND IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Auto-detect environment\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "\n",
    "if IS_KAGGLE:\n",
    "    print(\"üåê Running on Kaggle environment\")\n",
    "    # On Kaggle, repo is cloned to /kaggle/working/Final_exam\n",
    "    project_root = Path('/kaggle/working/Final_exam')\n",
    "else:\n",
    "    print(\"üíª Running on local environment\")\n",
    "    # On local, notebook is in notebooks/, so go up one level\n",
    "    project_root = Path.cwd().parent\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"üìÇ Project root: {project_root}\")\n",
    "print(f\"‚úì Python path updated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a1484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configurations and modules\n",
    "from config import (\n",
    "    EUROSAT_PATH, METADATA_PATH, BIGEARTHNET_FOLDERS, \n",
    "    REFERENCE_MAPS_FOLDER, CORINE_TO_EUROSAT, CLASS_NAMES, NUM_CLASSES,\n",
    "    IS_KAGGLE\n",
    ")\n",
    "from src.data import (\n",
    "    EuroSATDataset, BigEarthNetSegmentationDataset,\n",
    "    get_classification_train_augmentation, get_val_augmentation,\n",
    "    get_segmentation_train_augmentation\n",
    ")\n",
    "from src.utils.visualization import mask_to_rgb, denormalize_image, COLOR_PALETTE\n",
    "\n",
    "print(\"‚úì Imports successful!\")\n",
    "print(f\"\\nüìç Paths configuration:\")\n",
    "print(f\"  EuroSAT: {EUROSAT_PATH}\")\n",
    "print(f\"  Exists: {EUROSAT_PATH.exists()}\")\n",
    "print(f\"\\n  Metadata: {METADATA_PATH}\")\n",
    "print(f\"  Exists: {METADATA_PATH.exists()}\")\n",
    "print(f\"\\n  Reference Maps: {REFERENCE_MAPS_FOLDER}\")\n",
    "print(f\"  Exists: {REFERENCE_MAPS_FOLDER.exists()}\")\n",
    "print(f\"\\n  BigEarthNet folders: {len(BIGEARTHNET_FOLDERS)} found\")\n",
    "for folder in BIGEARTHNET_FOLDERS:\n",
    "    print(f\"    - {folder} (exists: {folder.exists()})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce3c26d",
   "metadata": {},
   "source": [
    "## üåç Test 1: EuroSAT Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba7d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTING EUROSAT DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create transforms\n",
    "train_transform = get_classification_train_augmentation(64, strength='light')\n",
    "val_transform = get_val_augmentation(64)\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    train_dataset = EuroSATDataset(EUROSAT_PATH, 'train', train_transform)\n",
    "    val_dataset = EuroSATDataset(EUROSAT_PATH, 'val', val_transform)\n",
    "    test_dataset = EuroSATDataset(EUROSAT_PATH, 'test', val_transform)\n",
    "    \n",
    "    print(f\"‚úì Train samples: {len(train_dataset):,}\")\n",
    "    print(f\"‚úì Val samples: {len(val_dataset):,}\")\n",
    "    print(f\"‚úì Test samples: {len(test_dataset):,}\")\n",
    "    print(f\"‚úì Classes: {train_dataset.classes}\")\n",
    "    \n",
    "    # Get class distribution\n",
    "    print(\"\\nüìä Class distribution:\")\n",
    "    train_dataset.get_class_distribution()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error loading EuroSAT: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d0ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize EuroSAT samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('EuroSAT Training Samples', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i in range(10):\n",
    "    sample = train_dataset[i]\n",
    "    img = sample['image']  # torch.Tensor (3, 64, 64)\n",
    "    label = sample['label']\n",
    "    class_name = CLASS_NAMES[label]\n",
    "    \n",
    "    # Denormalize - returns numpy array (H, W, 3)\n",
    "    img_np = denormalize_image(img)\n",
    "    \n",
    "    ax = axes[i // 5, i % 5]\n",
    "    ax.imshow(img_np)\n",
    "    ax.set_title(f'{class_name}', fontsize=10, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualized 10 EuroSAT samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ed3f9",
   "metadata": {},
   "source": [
    "## üõ∞Ô∏è Test 2: BigEarthNet Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8322d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LOADING BIGEARTHNET METADATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    metadata_df = pd.read_parquet(METADATA_PATH)\n",
    "    print(f\"‚úì Loaded metadata: {len(metadata_df):,} patches\")\n",
    "    print(f\"  Columns: {list(metadata_df.columns)}\")\n",
    "    \n",
    "    # Show split distribution\n",
    "    print(f\"\\nüìä Split distribution:\")\n",
    "    print(metadata_df['split'].value_counts())\n",
    "    \n",
    "    # For testing, use a subset\n",
    "    TEST_SUBSET_SIZE = 100  # Adjust based on your needs\n",
    "    \n",
    "    train_df = metadata_df[metadata_df['split'] == 'train'].head(TEST_SUBSET_SIZE)\n",
    "    val_df = metadata_df[metadata_df['split'] == 'validation'].head(50)\n",
    "    \n",
    "    print(f\"\\n‚úì Using {len(train_df)} train samples (subset for testing)\")\n",
    "    print(f\"‚úì Using {len(val_df)} val samples (subset for testing)\")\n",
    "    \n",
    "    # Show sample metadata\n",
    "    print(f\"\\nüìã Sample metadata:\")\n",
    "    print(train_df.head(3))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error loading metadata: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e59dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CREATING BIGEARTHNET DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create transforms\n",
    "seg_train_transform = get_segmentation_train_augmentation(120, strength='light')\n",
    "seg_val_transform = get_val_augmentation(120)\n",
    "\n",
    "try:\n",
    "    train_dataset_ben = BigEarthNetSegmentationDataset(\n",
    "        metadata_df=train_df,\n",
    "        data_folders=BIGEARTHNET_FOLDERS,\n",
    "        reference_maps_folder=REFERENCE_MAPS_FOLDER,\n",
    "        corine_to_eurosat_mapping=CORINE_TO_EUROSAT,\n",
    "        transform=seg_train_transform,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        validate_data=True\n",
    "    )\n",
    "    \n",
    "    val_dataset_ben = BigEarthNetSegmentationDataset(\n",
    "        metadata_df=val_df,\n",
    "        data_folders=BIGEARTHNET_FOLDERS,\n",
    "        reference_maps_folder=REFERENCE_MAPS_FOLDER,\n",
    "        corine_to_eurosat_mapping=CORINE_TO_EUROSAT,\n",
    "        transform=seg_val_transform,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        validate_data=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Created BigEarthNet train dataset: {len(train_dataset_ben)} samples\")\n",
    "    print(f\"‚úì Created BigEarthNet val dataset: {len(val_dataset_ben)} samples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error creating BigEarthNet dataset: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600dafb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TESTING SAMPLE LOADING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test loading a few samples\n",
    "num_test = 5\n",
    "success = 0\n",
    "\n",
    "for i in range(num_test):\n",
    "    try:\n",
    "        sample = train_dataset_ben[i]\n",
    "        print(f\"\\n‚úì Sample {i}: {sample['patch_id']}\")\n",
    "        print(f\"  Image: {sample['image'].shape}, Mask: {sample['mask'].shape}\")\n",
    "        print(f\"  Classes: {torch.unique(sample['mask']).tolist()}\")\n",
    "        success += 1\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó Sample {i} failed: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Loaded {success}/{num_test} samples successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571339c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize BigEarthNet samples\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "fig.suptitle('BigEarthNet Samples (Image & Mask)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i in range(6):\n",
    "    try:\n",
    "        sample = val_dataset_ben[i]\n",
    "        img = sample['image']  # torch.Tensor (3, 120, 120)\n",
    "        mask = sample['mask']  # torch.Tensor (120, 120)\n",
    "        \n",
    "        # Denormalize image - returns numpy array (H, W, 3)\n",
    "        img_np = denormalize_image(img)\n",
    "        \n",
    "        # Convert mask to RGB\n",
    "        if isinstance(mask, torch.Tensor):\n",
    "            mask = mask.cpu().numpy()\n",
    "        mask_rgb = mask_to_rgb(mask, COLOR_PALETTE)\n",
    "        \n",
    "        # Plot\n",
    "        ax_img = axes[i // 2, (i % 2) * 2]\n",
    "        ax_img.imshow(img_np)\n",
    "        ax_img.set_title(f'Sample {i}: Image', fontsize=10)\n",
    "        ax_img.axis('off')\n",
    "        \n",
    "        ax_mask = axes[i // 2, (i % 2) * 2 + 1]\n",
    "        ax_mask.imshow(mask_rgb)\n",
    "        ax_mask.set_title(f'Sample {i}: Mask', fontsize=10)\n",
    "        ax_mask.axis('off')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Failed to visualize sample {i}: {e}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"‚úì Visualization complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43675e6",
   "metadata": {},
   "source": [
    "## üîÑ Test 3: DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7514c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TESTING DATALOADER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_ben,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Use 0 for debugging\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# Get one batch\n",
    "try:\n",
    "    batch = next(iter(train_loader))\n",
    "    \n",
    "    print(f\"‚úì Batch loaded successfully!\")\n",
    "    print(f\"  Images: {batch['image'].shape}\")\n",
    "    print(f\"  Masks: {batch['mask'].shape}\")\n",
    "    print(f\"  Patch IDs: {batch['patch_id']}\")\n",
    "    print(f\"\\nüìä Data info:\")\n",
    "    print(f\"  Image dtype: {batch['image'].dtype}\")\n",
    "    print(f\"  Mask dtype: {batch['mask'].dtype}\")\n",
    "    print(f\"  Image range: [{batch['image'].min():.3f}, {batch['image'].max():.3f}]\")\n",
    "    print(f\"  Mask range: [{batch['mask'].min()}, {batch['mask'].max()}]\")\n",
    "    print(f\"  Unique classes: {torch.unique(batch['mask']).tolist()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e101d1",
   "metadata": {},
   "source": [
    "## üé® Class Color Legend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce6614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.axis('off')\n",
    "\n",
    "# Create legend patches\n",
    "legend_patches = []\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    color = COLOR_PALETTE[i] / 255.0\n",
    "    patch = Patch(color=color, label=f'{i}: {class_name}')\n",
    "    legend_patches.append(patch)\n",
    "\n",
    "ax.legend(handles=legend_patches, loc='center', fontsize=12, frameon=True)\n",
    "ax.set_title('Class Color Legend', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a6bd57",
   "metadata": {},
   "source": [
    "## üìã Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af43c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DATASET PIPELINE TEST SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüåê Environment: {'Kaggle' if IS_KAGGLE else 'Local'}\")\n",
    "\n",
    "print(\"\\n‚úÖ EuroSAT Dataset:\")\n",
    "try:\n",
    "    print(f\"  - Train: {len(train_dataset):,} samples\")\n",
    "    print(f\"  - Val: {len(val_dataset):,} samples\")\n",
    "    print(f\"  - Test: {len(test_dataset):,} samples\")\n",
    "    print(f\"  - Image size: 64x64\")\n",
    "except:\n",
    "    print(\"  ‚ö†Ô∏è  Dataset not loaded\")\n",
    "\n",
    "print(\"\\n‚úÖ BigEarthNet Dataset:\")\n",
    "try:\n",
    "    print(f\"  - Train: {len(train_dataset_ben)} samples (subset)\")\n",
    "    print(f\"  - Val: {len(val_dataset_ben)} samples (subset)\")\n",
    "    print(f\"  - Image size: 120x120\")\n",
    "    print(f\"  - Classes: {NUM_CLASSES}\")\n",
    "    \n",
    "    failed = train_dataset_ben.get_failed_samples()\n",
    "    if failed:\n",
    "        print(f\"  ‚ö†Ô∏è  Failed samples: {len(failed)}\")\n",
    "except:\n",
    "    print(\"  ‚ö†Ô∏è  Dataset not loaded\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ All tests completed!\")\n",
    "print(\"üöÄ Ready for training!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65941f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
