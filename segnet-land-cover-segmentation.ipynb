{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Đề bài: Sử dụng mạng học sâu để phân loại hình ảnh đất đai từ ảnh vệ tinh\n",
        "## Model: SegNet\n",
        "## Data: EuroSat-RGB + BigEarthNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm -rf /kaggle/working/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Clone repo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone từ nhánh khác (thay 'branch-name' bằng tên nhánh bạn muốn)\n",
        "# Ví dụ: nhánh 'segnet', 'dev', 'feature/segnet', v.v.\n",
        "BRANCH_NAME = \"segnet\"  # Thay đổi tên nhánh ở đây\n",
        "\n",
        "# Cách 1: Clone trực tiếp từ branch (nếu branch tồn tại)\n",
        "import os\n",
        "os.system(f'git clone -b {BRANCH_NAME} https://github.com/yuh-tech/ADCV_2025 /kaggle/working//Final_exam')\n",
        "\n",
        "# Cách 2: Nếu cách 1 không hoạt động, clone từ main rồi checkout sang branch khác:\n",
        "# !git clone https://github.com/yuh-tech/ADCV_2025 /kaggle/working//Final_exam\n",
        "# %cd /kaggle/working//Final_exam\n",
        "# !git checkout {BRANCH_NAME}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Cài đặt dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r /kaggle/working/Final_exam/requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python /kaggle/working/Final_exam/setup_kaggle.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Setup và Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SETUP AND IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Auto-detect environment\n",
        "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
        "\n",
        "if IS_KAGGLE:\n",
        "    print(\"Running on Kaggle environment\")\n",
        "    # On Kaggle, repo is cloned to /kaggle/working/Final_exam\n",
        "    project_root = Path('/kaggle/working/Final_exam')\n",
        "else:\n",
        "    print(\"Running on local environment\")\n",
        "    # On local, notebook is in notebooks/, so go up one level\n",
        "    project_root = Path.cwd().parent\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Python path updated\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import configurations and modules\n",
        "from config import (\n",
        "    EUROSAT_PATH, METADATA_PATH, BIGEARTHNET_FOLDERS, \n",
        "    REFERENCE_MAPS_FOLDER, CORINE_TO_EUROSAT, CLASS_NAMES, NUM_CLASSES,\n",
        "    IS_KAGGLE\n",
        ")\n",
        "from src.data import (\n",
        "    EuroSATDataset, BigEarthNetSegmentationDataset,\n",
        "    get_classification_train_augmentation, get_val_augmentation,\n",
        "    get_segmentation_train_augmentation\n",
        ")\n",
        "from src.models.segnet import SegNet, SegNetWithPretrainedEncoder\n",
        "from src.utils.visualization import mask_to_rgb, denormalize_image, COLOR_PALETTE\n",
        "\n",
        "print(\"Imports successful!\")\n",
        "print(f\"\\n Paths configuration:\")\n",
        "print(f\"  EuroSAT: {EUROSAT_PATH}\")\n",
        "print(f\"  Exists: {EUROSAT_PATH.exists()}\")\n",
        "print(f\"\\n  Metadata: {METADATA_PATH}\")\n",
        "print(f\"  Exists: {METADATA_PATH.exists()}\")\n",
        "print(f\"\\n  Reference Maps: {REFERENCE_MAPS_FOLDER}\")\n",
        "print(f\"  Exists: {REFERENCE_MAPS_FOLDER.exists()}\")\n",
        "print(f\"\\n  BigEarthNet folders: {len(BIGEARTHNET_FOLDERS)} found\")\n",
        "for folder in BIGEARTHNET_FOLDERS:\n",
        "    print(f\"    - {folder} (exists: {folder.exists()})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test SegNet Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test SegNet model creation\n",
        "print(\"=\"*70)\n",
        "print(\"TESTING SEGNET MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test standard SegNet\n",
        "print(\"\\n1. Standard SegNet (without pretrained encoder):\")\n",
        "model_standard = SegNet(\n",
        "    in_channels=3,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    base_channels=64\n",
        ")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model_standard.parameters())\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randn(2, 3, 120, 120)\n",
        "with torch.no_grad():\n",
        "    output = model_standard(test_input)\n",
        "print(f\"  Input shape: {test_input.shape}\")\n",
        "print(f\"  Output shape: {output.shape}\")\n",
        "print(f\"  ✓ Standard SegNet works!\")\n",
        "\n",
        "# Test SegNet with pretrained encoder\n",
        "print(\"\\n2. SegNet with Pretrained Encoder:\")\n",
        "model_pretrained = SegNetWithPretrainedEncoder(\n",
        "    encoder_name='resnet50',\n",
        "    num_classes=NUM_CLASSES,\n",
        "    encoder_pretrained=True,\n",
        "    freeze_encoder=False\n",
        ")\n",
        "\n",
        "total_params_pretrained = sum(p.numel() for p in model_pretrained.parameters())\n",
        "print(f\"  Total parameters: {total_params_pretrained:,}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_pretrained = model_pretrained(test_input)\n",
        "print(f\"  Input shape: {test_input.shape}\")\n",
        "print(f\"  Output shape: {output_pretrained.shape}\")\n",
        "print(f\"  ✓ SegNet with pretrained encoder works!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ SegNet model architecture verified!\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"TESTING EUROSAT DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create transforms\n",
        "train_transform = get_classification_train_augmentation(64, strength='light')\n",
        "val_transform = get_val_augmentation(64)\n",
        "\n",
        "# Load datasets\n",
        "try:\n",
        "    train_dataset = EuroSATDataset(EUROSAT_PATH, 'train', train_transform)\n",
        "    val_dataset = EuroSATDataset(EUROSAT_PATH, 'val', val_transform)\n",
        "    test_dataset = EuroSATDataset(EUROSAT_PATH, 'test', val_transform)\n",
        "    \n",
        "    print(f\"✓ Train samples: {len(train_dataset):,}\")\n",
        "    print(f\"✓ Val samples: {len(val_dataset):,}\")\n",
        "    print(f\"✓ Test samples: {len(test_dataset):,}\")\n",
        "    print(f\"✓ Classes: {train_dataset.classes}\")\n",
        "    \n",
        "    # Get class distribution\n",
        "    print(\"\\n Class distribution:\")\n",
        "    train_dataset.get_class_distribution()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading EuroSAT: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"LOADING BIGEARTHNET METADATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "try:\n",
        "    metadata_df = pd.read_parquet(METADATA_PATH)\n",
        "    print(f\"✓ Loaded metadata: {len(metadata_df):,} patches\")\n",
        "    print(f\"  Columns: {list(metadata_df.columns)}\")\n",
        "    \n",
        "    # Show split distribution\n",
        "    print(f\"\\n Split distribution:\")\n",
        "    print(metadata_df['split'].value_counts())\n",
        "    \n",
        "    # For testing, use a subset\n",
        "    TEST_SUBSET_SIZE = 100  # Adjust based on your needs\n",
        "    \n",
        "    train_df = metadata_df[metadata_df['split'] == 'train'].head(TEST_SUBSET_SIZE)\n",
        "    val_df = metadata_df[metadata_df['split'] == 'validation'].head(50)\n",
        "    \n",
        "    print(f\"\\n Using {len(train_df)} train samples (subset for testing)\")\n",
        "    print(f\" Using {len(val_df)} val samples (subset for testing)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading metadata: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"CREATING BIGEARTHNET DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create transforms\n",
        "seg_train_transform = get_segmentation_train_augmentation(120, strength='light')\n",
        "seg_val_transform = get_val_augmentation(120)\n",
        "\n",
        "try:\n",
        "    train_dataset_ben = BigEarthNetSegmentationDataset(\n",
        "        metadata_df=train_df,\n",
        "        data_folders=BIGEARTHNET_FOLDERS,\n",
        "        reference_maps_folder=REFERENCE_MAPS_FOLDER,\n",
        "        corine_to_eurosat_mapping=CORINE_TO_EUROSAT,\n",
        "        transform=seg_train_transform,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        validate_data=True\n",
        "    )\n",
        "    \n",
        "    val_dataset_ben = BigEarthNetSegmentationDataset(\n",
        "        metadata_df=val_df,\n",
        "        data_folders=BIGEARTHNET_FOLDERS,\n",
        "        reference_maps_folder=REFERENCE_MAPS_FOLDER,\n",
        "        corine_to_eurosat_mapping=CORINE_TO_EUROSAT,\n",
        "        transform=seg_val_transform,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        validate_data=True\n",
        "    )\n",
        "    \n",
        "    print(f\"✓ Created BigEarthNet train dataset: {len(train_dataset_ben)} samples\")\n",
        "    print(f\"✓ Created BigEarthNet val dataset: {len(val_dataset_ben)} samples\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error creating BigEarthNet dataset: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TESTING DATALOADER WITH SEGNET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(\n",
        "    train_dataset_ben,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=0,  # Use 0 for debugging\n",
        "    pin_memory=False\n",
        ")\n",
        "\n",
        "# Get one batch\n",
        "try:\n",
        "    batch = next(iter(train_loader))\n",
        "    \n",
        "    print(f\"✓ Batch loaded successfully!\")\n",
        "    print(f\"  Images: {batch['image'].shape}\")\n",
        "    print(f\"  Masks: {batch['mask'].shape}\")\n",
        "    print(f\"  Patch IDs: {batch['patch_id']}\")\n",
        "    \n",
        "    # Test SegNet forward pass with real data\n",
        "    print(f\"\\n Testing SegNet forward pass...\")\n",
        "    model_test = SegNetWithPretrainedEncoder(\n",
        "        encoder_name='resnet50',\n",
        "        num_classes=NUM_CLASSES,\n",
        "        encoder_pretrained=True\n",
        "    )\n",
        "    model_test.eval()\n",
        "    \n",
        "    images = batch['image']\n",
        "    with torch.no_grad():\n",
        "        predictions = model_test(images)\n",
        "    \n",
        "    print(f\"  Input images: {images.shape}\")\n",
        "    print(f\"  Predictions: {predictions.shape}\")\n",
        "    print(f\"  Masks: {batch['mask'].shape}\")\n",
        "    print(f\"  ✓ SegNet forward pass successful!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualize SegNet Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize SegNet predictions\n",
        "print(\"=\"*70)\n",
        "print(\"VISUALIZING SEGNET PREDICTIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from src.utils.visualization import visualize_segmentation\n",
        "\n",
        "# Get a sample batch\n",
        "sample_batch = next(iter(train_loader))\n",
        "images = sample_batch['image']\n",
        "masks = sample_batch['mask']\n",
        "\n",
        "# Create model and predict\n",
        "model_vis = SegNetWithPretrainedEncoder(\n",
        "    encoder_name='resnet50',\n",
        "    num_classes=NUM_CLASSES,\n",
        "    encoder_pretrained=True\n",
        ")\n",
        "model_vis.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model_vis(images)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
        "fig.suptitle('SegNet Predictions (Untrained Model)', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i in range(min(3, len(images))):\n",
        "    # Original image\n",
        "    img_np = denormalize_image(images[i])\n",
        "    axes[i, 0].imshow(img_np)\n",
        "    axes[i, 0].set_title('Original Image', fontweight='bold')\n",
        "    axes[i, 0].axis('off')\n",
        "    \n",
        "    # Ground truth mask\n",
        "    mask_np = mask_to_rgb(masks[i].numpy(), COLOR_PALETTE)\n",
        "    axes[i, 1].imshow(mask_np)\n",
        "    axes[i, 1].set_title('Ground Truth', fontweight='bold')\n",
        "    axes[i, 1].axis('off')\n",
        "    \n",
        "    # Prediction\n",
        "    pred_mask = torch.argmax(predictions[i], dim=0).cpu().numpy()\n",
        "    pred_mask_rgb = mask_to_rgb(pred_mask, COLOR_PALETTE)\n",
        "    axes[i, 2].imshow(pred_mask_rgb)\n",
        "    axes[i, 2].set_title('SegNet Prediction', fontweight='bold')\n",
        "    axes[i, 2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Visualization complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training SegNet\n",
        "\n",
        "Bạn có thể chạy training bằng cách:\n",
        "- **Local**: `python train_stage2_segnet.py --epochs 50`\n",
        "- **Kaggle**: `!python train_stage2_segnet.py --epochs 50 --batch-size 16`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
