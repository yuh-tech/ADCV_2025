{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ƒê·ªÅ b√†i: S·ª≠ d·ª•ng m·∫°ng h·ªçc s√¢u ƒë·ªÉ ph√¢n lo·∫°i h√¨nh ·∫£nh ƒë·∫•t ƒëai t·ª´ ·∫£nh v·ªá tinh\n",
        "## Model: SegNet\n",
        "## Data: EuroSat-RGB + BigEarthNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm -rf /kaggle/working/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Clone repo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone t·ª´ nh√°nh kh√°c (thay 'branch-name' b·∫±ng t√™n nh√°nh b·∫°n mu·ªën)\n",
        "# V√≠ d·ª•: nh√°nh 'segnet', 'dev', 'feature/segnet', v.v.\n",
        "BRANCH_NAME = \"segnet\"  # Thay ƒë·ªïi t√™n nh√°nh ·ªü ƒë√¢y\n",
        "\n",
        "# C√°ch 1: Clone tr·ª±c ti·∫øp t·ª´ branch (n·∫øu branch t·ªìn t·∫°i)\n",
        "import os\n",
        "os.system(f'git clone -b {BRANCH_NAME} https://github.com/yuh-tech/ADCV_2025 /kaggle/working//Final_exam')\n",
        "\n",
        "# C√°ch 2: N·∫øu c√°ch 1 kh√¥ng ho·∫°t ƒë·ªông, clone t·ª´ main r·ªìi checkout sang branch kh√°c:\n",
        "# !git clone https://github.com/yuh-tech/ADCV_2025 /kaggle/working//Final_exam\n",
        "# %cd /kaggle/working//Final_exam\n",
        "# !git checkout {BRANCH_NAME}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. C√†i ƒë·∫∑t dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -r /kaggle/working/Final_exam/requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python /kaggle/working/Final_exam/setup_kaggle.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Setup v√† Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SETUP AND IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Auto-detect environment\n",
        "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
        "\n",
        "if IS_KAGGLE:\n",
        "    print(\"Running on Kaggle environment\")\n",
        "    # On Kaggle, repo is cloned to /kaggle/working/Final_exam\n",
        "    project_root = Path('/kaggle/working/Final_exam')\n",
        "else:\n",
        "    print(\"Running on local environment\")\n",
        "    # On local, notebook is in notebooks/, so go up one level\n",
        "    project_root = Path.cwd().parent\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "print(f\"Project root: {project_root}\")\n",
        "print(f\"Python path updated\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import configurations and modules\n",
        "from config import (\n",
        "    EUROSAT_PATH, METADATA_PATH, BIGEARTHNET_FOLDERS, \n",
        "    REFERENCE_MAPS_FOLDER, CORINE_TO_EUROSAT, CLASS_NAMES, NUM_CLASSES,\n",
        "    IS_KAGGLE\n",
        ")\n",
        "from src.data import (\n",
        "    EuroSATDataset, BigEarthNetSegmentationDataset,\n",
        "    get_classification_train_augmentation, get_val_augmentation,\n",
        "    get_segmentation_train_augmentation\n",
        ")\n",
        "# Import SegNet models t·ª´ src/models/segnet.py\n",
        "# ƒê√¢y l√† file ch·ª©a SegNet architecture ƒë∆∞·ª£c s·ª≠ d·ª•ng trong training\n",
        "from src.models.segnet import SegNet, SegNetWithPretrainedEncoder\n",
        "from src.utils.visualization import mask_to_rgb, denormalize_image, COLOR_PALETTE\n",
        "\n",
        "print(\"Imports successful!\")\n",
        "print(f\"\\n Paths configuration:\")\n",
        "print(f\"  EuroSAT: {EUROSAT_PATH}\")\n",
        "print(f\"  Exists: {EUROSAT_PATH.exists()}\")\n",
        "print(f\"\\n  Metadata: {METADATA_PATH}\")\n",
        "print(f\"  Exists: {METADATA_PATH.exists()}\")\n",
        "print(f\"\\n  Reference Maps: {REFERENCE_MAPS_FOLDER}\")\n",
        "print(f\"  Exists: {REFERENCE_MAPS_FOLDER.exists()}\")\n",
        "print(f\"\\n  BigEarthNet folders: {len(BIGEARTHNET_FOLDERS)} found\")\n",
        "for folder in BIGEARTHNET_FOLDERS:\n",
        "    print(f\"    - {folder} (exists: {folder.exists()})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Test SegNet Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test SegNet model creation\n",
        "print(\"=\"*70)\n",
        "print(\"TESTING SEGNET MODEL\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Test standard SegNet\n",
        "print(\"\\n1. Standard SegNet (without pretrained encoder):\")\n",
        "model_standard = SegNet(\n",
        "    in_channels=3,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    base_channels=64\n",
        ")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model_standard.parameters())\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randn(2, 3, 120, 120)\n",
        "with torch.no_grad():\n",
        "    output = model_standard(test_input)\n",
        "print(f\"  Input shape: {test_input.shape}\")\n",
        "print(f\"  Output shape: {output.shape}\")\n",
        "print(f\"  ‚úì Standard SegNet works!\")\n",
        "\n",
        "# Test SegNet with pretrained encoder\n",
        "print(\"\\n2. SegNet with Pretrained Encoder:\")\n",
        "model_pretrained = SegNetWithPretrainedEncoder(\n",
        "    encoder_name='resnet50',\n",
        "    num_classes=NUM_CLASSES,\n",
        "    encoder_pretrained=True,\n",
        "    freeze_encoder=False\n",
        ")\n",
        "\n",
        "total_params_pretrained = sum(p.numel() for p in model_pretrained.parameters())\n",
        "print(f\"  Total parameters: {total_params_pretrained:,}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_pretrained = model_pretrained(test_input)\n",
        "print(f\"  Input shape: {test_input.shape}\")\n",
        "print(f\"  Output shape: {output_pretrained.shape}\")\n",
        "print(f\"  ‚úì SegNet with pretrained encoder works!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ SegNet model architecture verified!\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"TESTING EUROSAT DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create transforms\n",
        "train_transform = get_classification_train_augmentation(64, strength='light')\n",
        "val_transform = get_val_augmentation(64)\n",
        "\n",
        "# Load datasets\n",
        "try:\n",
        "    train_dataset = EuroSATDataset(EUROSAT_PATH, 'train', train_transform)\n",
        "    val_dataset = EuroSATDataset(EUROSAT_PATH, 'val', val_transform)\n",
        "    test_dataset = EuroSATDataset(EUROSAT_PATH, 'test', val_transform)\n",
        "    \n",
        "    print(f\"‚úì Train samples: {len(train_dataset):,}\")\n",
        "    print(f\"‚úì Val samples: {len(val_dataset):,}\")\n",
        "    print(f\"‚úì Test samples: {len(test_dataset):,}\")\n",
        "    print(f\"‚úì Classes: {train_dataset.classes}\")\n",
        "    \n",
        "    # Get class distribution\n",
        "    print(\"\\n Class distribution:\")\n",
        "    train_dataset.get_class_distribution()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\" Error loading EuroSAT: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"LOADING BIGEARTHNET METADATA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "try:\n",
        "    metadata_df = pd.read_parquet(METADATA_PATH)\n",
        "    print(f\" Loaded metadata: {len(metadata_df):,} patches\")\n",
        "    print(f\"  Columns: {list(metadata_df.columns)}\")\n",
        "    \n",
        "    # Show split distribution\n",
        "    print(f\"\\n Split distribution:\")\n",
        "    print(metadata_df['split'].value_counts())\n",
        "    \n",
        "    # For testing, use a subset\n",
        "    TEST_SUBSET_SIZE = 100  # Adjust based on your needs\n",
        "    \n",
        "    train_df = metadata_df[metadata_df['split'] == 'train'].head(TEST_SUBSET_SIZE)\n",
        "    val_df = metadata_df[metadata_df['split'] == 'validation'].head(50)\n",
        "    \n",
        "    print(f\"\\n Using {len(train_df)} train samples (subset for testing)\")\n",
        "    print(f\" Using {len(val_df)} val samples (subset for testing)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\" Error loading metadata: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"CREATING BIGEARTHNET DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create transforms\n",
        "seg_train_transform = get_segmentation_train_augmentation(120, strength='light')\n",
        "seg_val_transform = get_val_augmentation(120)\n",
        "\n",
        "try:\n",
        "    train_dataset_ben = BigEarthNetSegmentationDataset(\n",
        "        metadata_df=train_df,\n",
        "        data_folders=BIGEARTHNET_FOLDERS,\n",
        "        reference_maps_folder=REFERENCE_MAPS_FOLDER,\n",
        "        corine_to_eurosat_mapping=CORINE_TO_EUROSAT,\n",
        "        transform=seg_train_transform,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        validate_data=True\n",
        "    )\n",
        "    \n",
        "    val_dataset_ben = BigEarthNetSegmentationDataset(\n",
        "        metadata_df=val_df,\n",
        "        data_folders=BIGEARTHNET_FOLDERS,\n",
        "        reference_maps_folder=REFERENCE_MAPS_FOLDER,\n",
        "        corine_to_eurosat_mapping=CORINE_TO_EUROSAT,\n",
        "        transform=seg_val_transform,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        validate_data=True\n",
        "    )\n",
        "    \n",
        "    print(f\" Created BigEarthNet train dataset: {len(train_dataset_ben)} samples\")\n",
        "    print(f\" Created BigEarthNet val dataset: {len(val_dataset_ben)} samples\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\" Error creating BigEarthNet dataset: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TESTING DATALOADER WITH SEGNET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader = DataLoader(\n",
        "    train_dataset_ben,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    num_workers=0,  # Use 0 for debugging\n",
        "    pin_memory=False\n",
        ")\n",
        "\n",
        "# Get one batch\n",
        "try:\n",
        "    batch = next(iter(train_loader))\n",
        "    \n",
        "    print(f\" Batch loaded successfully!\")\n",
        "    print(f\"  Images: {batch['image'].shape}\")\n",
        "    print(f\"  Masks: {batch['mask'].shape}\")\n",
        "    print(f\"  Patch IDs: {batch['patch_id']}\")\n",
        "    \n",
        "    # Test SegNet forward pass with real data\n",
        "    print(f\"\\n Testing SegNet forward pass...\")\n",
        "    model_test = SegNetWithPretrainedEncoder(\n",
        "        encoder_name='resnet50',\n",
        "        num_classes=NUM_CLASSES,\n",
        "        encoder_pretrained=True\n",
        "    )\n",
        "    model_test.eval()\n",
        "    \n",
        "    images = batch['image']\n",
        "    with torch.no_grad():\n",
        "        predictions = model_test(images)\n",
        "    \n",
        "    print(f\"  Input images: {images.shape}\")\n",
        "    print(f\"  Predictions: {predictions.shape}\")\n",
        "    print(f\"  Masks: {batch['mask'].shape}\")\n",
        "    print(f\"   SegNet forward pass successful!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\" Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualize SegNet Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize SegNet predictions\n",
        "print(\"=\"*70)\n",
        "print(\"VISUALIZING SEGNET PREDICTIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from src.utils.visualization import visualize_segmentation\n",
        "\n",
        "# Get a sample batch\n",
        "sample_batch = next(iter(train_loader))\n",
        "images = sample_batch['image']\n",
        "masks = sample_batch['mask']\n",
        "\n",
        "# Create model and predict\n",
        "model_vis = SegNetWithPretrainedEncoder(\n",
        "    encoder_name='resnet50',\n",
        "    num_classes=NUM_CLASSES,\n",
        "    encoder_pretrained=True\n",
        ")\n",
        "model_vis.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model_vis(images)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
        "fig.suptitle('SegNet Predictions (Untrained Model)', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i in range(min(3, len(images))):\n",
        "    # Original image\n",
        "    img_np = denormalize_image(images[i])\n",
        "    axes[i, 0].imshow(img_np)\n",
        "    axes[i, 0].set_title('Original Image', fontweight='bold')\n",
        "    axes[i, 0].axis('off')\n",
        "    \n",
        "    # Ground truth mask\n",
        "    mask_np = mask_to_rgb(masks[i].numpy(), COLOR_PALETTE)\n",
        "    axes[i, 1].imshow(mask_np)\n",
        "    axes[i, 1].set_title('Ground Truth', fontweight='bold')\n",
        "    axes[i, 1].axis('off')\n",
        "    \n",
        "    # Prediction\n",
        "    pred_mask = torch.argmax(predictions[i], dim=0).cpu().numpy()\n",
        "    pred_mask_rgb = mask_to_rgb(pred_mask, COLOR_PALETTE)\n",
        "    axes[i, 2].imshow(pred_mask_rgb)\n",
        "    axes[i, 2].set_title('SegNet Prediction', fontweight='bold')\n",
        "    axes[i, 2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\" Visualization complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Pipeline\n",
        "\n",
        "### 7.1. Stage 1: Pre-train Encoder v·ªõi EuroSAT\n",
        "\n",
        "S·ª≠ d·ª•ng script `train_stage1.py` ƒë·ªÉ train encoder v·ªõi EuroSAT dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ch·∫°y Stage 1 training s·ª≠ d·ª•ng script train_stage1.py\n",
        "# B·∫°n c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh c√°c tham s·ªë: --epochs, --batch-size, --lr, --model\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STAGE 1: ENCODER PRE-TRAINING WITH EUROSAT\")\n",
        "print(\"=\"*70)\n",
        "print(\"S·ª≠ d·ª•ng script: train_stage1.py\")\n",
        "print(\"\\nC√°c tham s·ªë c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh:\")\n",
        "print(\"  --epochs: S·ªë epochs (m·∫∑c ƒë·ªãnh: 50)\")\n",
        "print(\"  --batch-size: Batch size (m·∫∑c ƒë·ªãnh: 64)\")\n",
        "print(\"  --lr: Learning rate (m·∫∑c ƒë·ªãnh: 0.001)\")\n",
        "print(\"  --model: Model name (m·∫∑c ƒë·ªãnh: resnet50)\")\n",
        "\n",
        "# Ch·∫°y training\n",
        "!cd {project_root} && python train_stage1.py --epochs 50 --batch-size 64\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra k·∫øt qu·∫£ Stage 1 (trong m√¥i tr∆∞·ªùng Kaggle)\n",
        "import os\n",
        "from datetime import datetime\n",
        "from config import CHECKPOINTS_DIR, VISUALIZATIONS_DIR, LOGS_DIR\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"KI·ªÇM TRA K·∫æT QU·∫¢ STAGE 1 (KAGGLE ENVIRONMENT)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ƒê∆∞·ªùng d·∫´n trong Kaggle\n",
        "if IS_KAGGLE:\n",
        "    print(f\" Running on Kaggle environment\")\n",
        "    print(f\" Project root: {project_root}\")\n",
        "else:\n",
        "    print(f\" Running on local environment\")\n",
        "    print(f\" Project root: {project_root}\")\n",
        "\n",
        "stage1_checkpoint_dir = CHECKPOINTS_DIR / 'stage1'\n",
        "best_model_stage1 = stage1_checkpoint_dir / 'best_model.pth'\n",
        "encoder_weights = stage1_checkpoint_dir / 'encoder_pretrained.pth'\n",
        "log_file = LOGS_DIR / 'stage1_training.log'\n",
        "\n",
        "# Helper function to format file size\n",
        "def format_size(size_bytes):\n",
        "    \"\"\"Convert bytes to human readable format.\"\"\"\n",
        "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
        "        if size_bytes < 1024.0:\n",
        "            return f\"{size_bytes:.2f} {unit}\"\n",
        "        size_bytes /= 1024.0\n",
        "    return f\"{size_bytes:.2f} TB\"\n",
        "\n",
        "# Helper function to get file modification time\n",
        "def get_file_time(file_path):\n",
        "    \"\"\"Get file modification time.\"\"\"\n",
        "    if file_path.exists():\n",
        "        mtime = os.path.getmtime(file_path)\n",
        "        return datetime.fromtimestamp(mtime).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    return None\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"1. CHECKPOINT FILES\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Check best model\n",
        "if best_model_stage1.exists():\n",
        "    size = os.path.getsize(best_model_stage1)\n",
        "    mtime = get_file_time(best_model_stage1)\n",
        "    print(f\"‚úì Best model found:\")\n",
        "    print(f\"  Path: {best_model_stage1}\")\n",
        "    print(f\"  Size: {format_size(size)}\")\n",
        "    print(f\"  Modified: {mtime}\")\n",
        "else:\n",
        "    print(f\"‚úó Best model NOT found:\")\n",
        "    print(f\"  Path: {best_model_stage1}\")\n",
        "    print(f\"  Expected at: {best_model_stage1.absolute()}\")\n",
        "\n",
        "# Check encoder weights\n",
        "if encoder_weights.exists():\n",
        "    size = os.path.getsize(encoder_weights)\n",
        "    mtime = get_file_time(encoder_weights)\n",
        "    print(f\"\\n‚úì Encoder weights found:\")\n",
        "    print(f\"  Path: {encoder_weights}\")\n",
        "    print(f\"  Size: {format_size(size)}\")\n",
        "    print(f\"  Modified: {mtime}\")\n",
        "else:\n",
        "    print(f\"\\n‚úó Encoder weights NOT found:\")\n",
        "    print(f\"  Path: {encoder_weights}\")\n",
        "    print(f\"  Expected at: {encoder_weights.absolute()}\")\n",
        "\n",
        "# Load checkpoint info if exists\n",
        "if best_model_stage1.exists():\n",
        "    try:\n",
        "        checkpoint = torch.load(best_model_stage1, map_location='cpu')\n",
        "        print(f\"\\n Checkpoint Info:\")\n",
        "        if 'epoch' in checkpoint:\n",
        "            print(f\"  Best epoch: {checkpoint['epoch']}\")\n",
        "        if 'val_accuracy' in checkpoint:\n",
        "            print(f\"  Best validation accuracy: {checkpoint['val_accuracy']:.4f} ({checkpoint['val_accuracy']*100:.2f}%)\")\n",
        "        if 'val_loss' in checkpoint:\n",
        "            print(f\"  Best validation loss: {checkpoint['val_loss']:.4f}\")\n",
        "        if 'train_accuracy' in checkpoint:\n",
        "            print(f\"  Training accuracy: {checkpoint['train_accuracy']:.4f}\")\n",
        "        if 'train_loss' in checkpoint:\n",
        "            print(f\"  Training loss: {checkpoint['train_loss']:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n Error loading checkpoint: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"2. TRAINING LOG FILE\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "if log_file.exists():\n",
        "    size = os.path.getsize(log_file)\n",
        "    mtime = get_file_time(log_file)\n",
        "    print(f\" Training log found:\")\n",
        "    print(f\"  Path: {log_file}\")\n",
        "    print(f\"  Size: {format_size(size)}\")\n",
        "    print(f\"  Modified: {mtime}\")\n",
        "    \n",
        "    # Parse v√† hi·ªÉn th·ªã metrics t·ª´ log\n",
        "    print(f\"\\n Training Metrics from Log:\")\n",
        "    try:\n",
        "        with open(log_file, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "            \n",
        "        # T√¨m test results\n",
        "        test_results_found = False\n",
        "        for i, line in enumerate(lines):\n",
        "            if \"Test Results:\" in line:\n",
        "                test_results_found = True\n",
        "                print(\"\\n  Test Set Results:\")\n",
        "                # Print next few lines that contain metrics\n",
        "                for j in range(i+1, min(i+5, len(lines))):\n",
        "                    if \"Test\" in lines[j] or \"Accuracy\" in lines[j] or \"F1\" in lines[j] or \"Loss\" in lines[j]:\n",
        "                        print(f\"    {lines[j].strip()}\")\n",
        "                break\n",
        "        \n",
        "        if not test_results_found:\n",
        "            print(\"  (Test results not found in log yet)\")\n",
        "        \n",
        "        # T√¨m best validation accuracy\n",
        "        best_val_acc = None\n",
        "        best_epoch = None\n",
        "        for line in lines:\n",
        "            if \"Best\" in line and (\"validation\" in line.lower() or \"val\" in line.lower()) and \"accuracy\" in line.lower():\n",
        "                # Try to extract accuracy value\n",
        "                import re\n",
        "                match = re.search(r'accuracy[:\\s]+([\\d.]+)', line, re.IGNORECASE)\n",
        "                if match:\n",
        "                    best_val_acc = float(match.group(1))\n",
        "            if \"Best epoch\" in line or \"best epoch\" in line.lower():\n",
        "                import re\n",
        "                match = re.search(r'epoch[:\\s]+(\\d+)', line, re.IGNORECASE)\n",
        "                if match:\n",
        "                    best_epoch = int(match.group(1))\n",
        "        \n",
        "        if best_val_acc is not None:\n",
        "            print(f\"\\n  Best Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
        "        if best_epoch is not None:\n",
        "            print(f\"  Best Epoch: {best_epoch}\")\n",
        "        \n",
        "        # Show last few lines\n",
        "        print(f\"\\n  Last 10 lines from log:\")\n",
        "        for line in lines[-10:]:\n",
        "            if line.strip():  # Skip empty lines\n",
        "                print(f\"    {line.strip()}\")\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\" Error reading log file: {e}\")\n",
        "        # Fallback: just show last few lines\n",
        "        try:\n",
        "            with open(log_file, 'r', encoding='utf-8') as f:\n",
        "                lines = f.readlines()\n",
        "                print(f\"\\n  Last 10 lines:\")\n",
        "                for line in lines[-10:]:\n",
        "                    print(f\"    {line.strip()}\")\n",
        "        except:\n",
        "            pass\n",
        "else:\n",
        "    print(f\" Training log NOT found:\")\n",
        "    print(f\"  Path: {log_file}\")\n",
        "    print(f\"  Expected at: {log_file.absolute()}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"3. VISUALIZATION FILES\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "cm_path_stage1 = VISUALIZATIONS_DIR / 'stage1_confusion_matrix.png'\n",
        "curves_path_stage1 = VISUALIZATIONS_DIR / 'stage1_training_curves.png'\n",
        "\n",
        "for viz_file, name in [(cm_path_stage1, \"Confusion Matrix\"), (curves_path_stage1, \"Training Curves\")]:\n",
        "    if viz_file.exists():\n",
        "        size = os.path.getsize(viz_file)\n",
        "        mtime = get_file_time(viz_file)\n",
        "        print(f\" {name} found:\")\n",
        "        print(f\"  Path: {viz_file}\")\n",
        "        print(f\"  Size: {format_size(size)}\")\n",
        "        print(f\"  Modified: {mtime}\")\n",
        "    else:\n",
        "        print(f\"‚úó {name} NOT found:\")\n",
        "        print(f\"  Path: {viz_file}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "files_found = 0\n",
        "files_total = 4\n",
        "\n",
        "if best_model_stage1.exists():\n",
        "    files_found += 1\n",
        "if encoder_weights.exists():\n",
        "    files_found += 1\n",
        "if log_file.exists():\n",
        "    files_found += 1\n",
        "if cm_path_stage1.exists() or curves_path_stage1.exists():\n",
        "    files_found += 1\n",
        "\n",
        "print(f\" Files found: {files_found}/{files_total}\")\n",
        "\n",
        "if files_found == files_total:\n",
        "    print(\" Stage 1 training completed successfully!\")\n",
        "    print(\"   Ready to proceed to Stage 2 training.\")\n",
        "elif files_found > 0:\n",
        "    print(\"  Stage 1 training partially completed.\")\n",
        "    print(\"   Some files are missing. Check if training finished properly.\")\n",
        "else:\n",
        "    print(\" Stage 1 training outputs not found.\")\n",
        "    print(\"   Please run Stage 1 training first.\")\n",
        "\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Xem th√™m th√¥ng tin t·ª´ checkpoint Stage 1\n",
        "if best_model_stage1.exists():\n",
        "    checkpoint = torch.load(best_model_stage1, map_location='cpu')\n",
        "    print(\"\\n Stage 1 Checkpoint Info:\")\n",
        "    if 'epoch' in checkpoint:\n",
        "        print(f\"  Best epoch: {checkpoint['epoch']}\")\n",
        "    if 'val_accuracy' in checkpoint:\n",
        "        print(f\"  Best validation accuracy: {checkpoint['val_accuracy']:.4f}\")\n",
        "    if 'val_loss' in checkpoint:\n",
        "        print(f\"  Best validation loss: {checkpoint['val_loss']:.4f}\")\n",
        "    print(f\"  Checkpoint keys: {list(checkpoint.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load v√† visualize k·∫øt qu·∫£ Stage 1\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Loading Stage 1 Results for Visualization...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Ki·ªÉm tra c√°c file visualization\n",
        "cm_path_stage1 = VISUALIZATIONS_DIR / 'stage1_confusion_matrix.png'\n",
        "curves_path_stage1 = VISUALIZATIONS_DIR / 'stage1_training_curves.png'\n",
        "\n",
        "if cm_path_stage1.exists():\n",
        "    print(f\"‚úì Confusion matrix found: {cm_path_stage1}\")\n",
        "    img = plt.imread(cm_path_stage1)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title('Stage 1: Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\" Confusion matrix not found: {cm_path_stage1}\")\n",
        "\n",
        "if curves_path_stage1.exists():\n",
        "    print(f\"‚úì Training curves found: {curves_path_stage1}\")\n",
        "    img = plt.imread(curves_path_stage1)\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title('Stage 1: Training Curves', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\" Training curves not found: {curves_path_stage1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2. Stage 2: Train SegNet v·ªõi BigEarthNet\n",
        "\n",
        "S·ª≠ d·ª•ng script `train_stage2_segnet.py` ƒë·ªÉ train SegNet v·ªõi BigEarthNet dataset.\n",
        "**L∆∞u √Ω**: Script n√†y s·ª≠ d·ª•ng model SegNet t·ª´ `src/models/segnet.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ch·∫°y Stage 2 training s·ª≠ d·ª•ng script train_stage2_segnet.py\n",
        "# Script n√†y s·ª≠ d·ª•ng SegNetWithPretrainedEncoder t·ª´ src/models/segnet.py\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STAGE 2: SEGNET TRAINING WITH BIGEARTHNET\")\n",
        "print(\"=\"*70)\n",
        "print(\"S·ª≠ d·ª•ng script: train_stage2_segnet.py\")\n",
        "print(\"Model: SegNetWithPretrainedEncoder (t·ª´ src/models/segnet.py)\")\n",
        "print(\"\\nC√°c tham s·ªë c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh:\")\n",
        "print(\"  --epochs: S·ªë epochs (m·∫∑c ƒë·ªãnh: 50)\")\n",
        "print(\"  --batch-size: Batch size (m·∫∑c ƒë·ªãnh: 16)\")\n",
        "print(\"  --encoder-lr: Learning rate cho encoder (m·∫∑c ƒë·ªãnh: 1e-5)\")\n",
        "print(\"  --decoder-lr: Learning rate cho decoder (m·∫∑c ƒë·ªãnh: 1e-3)\")\n",
        "print(\"  --freeze-epochs: S·ªë epochs freeze encoder (m·∫∑c ƒë·ªãnh: 10)\")\n",
        "\n",
        "# Ki·ªÉm tra encoder weights t·ª´ Stage 1\n",
        "encoder_weights_path = CHECKPOINTS_DIR / 'stage1' / 'encoder_pretrained.pth'\n",
        "if encoder_weights_path.exists():\n",
        "    print(f\"\\n Encoder weights t·ª´ Stage 1: {encoder_weights_path}\")\n",
        "else:\n",
        "    print(f\"\\n Warning: Encoder weights kh√¥ng t√¨m th·∫•y t·∫°i {encoder_weights_path}\")\n",
        "    print(\"  Script s·∫Ω s·ª≠ d·ª•ng ImageNet pretrained weights thay th·∫ø\")\n",
        "\n",
        "# Ch·∫°y training\n",
        "!cd {project_root} && python train_stage2_segnet.py --epochs 50 --batch-size 16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra k·∫øt qu·∫£ Stage 2\n",
        "stage2_checkpoint_dir = CHECKPOINTS_DIR / 'stage2_segnet'\n",
        "best_model_stage2 = stage2_checkpoint_dir / 'best_model.pth'\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Ki·ªÉm tra k·∫øt qu·∫£ Stage 2...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if best_model_stage2.exists():\n",
        "    print(f\" Best SegNet model saved at: {best_model_stage2}\")\n",
        "else:\n",
        "    print(f\" Best model not found at: {best_model_stage2}\")\n",
        "\n",
        "# Load v√† hi·ªÉn th·ªã k·∫øt qu·∫£ t·ª´ log file\n",
        "log_file_stage2 = project_root / 'outputs' / 'logs' / 'stage2_segnet_training.log'\n",
        "if log_file_stage2.exists():\n",
        "    print(f\"\\n‚úì Training log saved at: {log_file_stage2}\")\n",
        "    # ƒê·ªçc v√† hi·ªÉn th·ªã m·ªôt ph·∫ßn log\n",
        "    with open(log_file_stage2, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        print(\"\\n Last few lines from training log:\")\n",
        "        for line in lines[-15:]:\n",
        "            print(f\"  {line.strip()}\")\n",
        "else:\n",
        "    print(f\"\\n Training log not found at: {log_file_stage2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Xem th√™m th√¥ng tin t·ª´ checkpoint Stage 2\n",
        "if best_model_stage2.exists():\n",
        "    checkpoint = torch.load(best_model_stage2, map_location='cpu')\n",
        "    print(\"\\nüìä Stage 2 SegNet Checkpoint Info:\")\n",
        "    if 'epoch' in checkpoint:\n",
        "        print(f\"  Best epoch: {checkpoint['epoch']}\")\n",
        "    if 'val_mean_iou' in checkpoint:\n",
        "        print(f\"  Best validation mIoU: {checkpoint['val_mean_iou']:.4f}\")\n",
        "    if 'val_pixel_accuracy' in checkpoint:\n",
        "        print(f\"  Best validation pixel accuracy: {checkpoint['val_pixel_accuracy']:.4f}\")\n",
        "    if 'val_loss' in checkpoint:\n",
        "        print(f\"  Best validation loss: {checkpoint['val_loss']:.4f}\")\n",
        "    print(f\"  Checkpoint keys: {list(checkpoint.keys())}\")\n",
        "    \n",
        "    # Verify model type\n",
        "    if 'model_state_dict' in checkpoint:\n",
        "        model_keys = list(checkpoint['model_state_dict'].keys())\n",
        "        # Check if it's SegNet (should have decoder layers)\n",
        "        has_segnet_decoder = any('decoder' in key or 'dec5' in key or 'dec4' in key for key in model_keys)\n",
        "        if has_segnet_decoder:\n",
        "            print(\"\\n Model confirmed: SegNet architecture detected in checkpoint\")\n",
        "        else:\n",
        "            print(\"\\n Warning: SegNet decoder layers not clearly detected in checkpoint\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualize Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load v√† hi·ªÉn th·ªã visualizations t·ª´ Stage 2\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Visualizing Stage 2 Results...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Confusion matrix\n",
        "cm_path_stage2 = VISUALIZATIONS_DIR / 'stage2_segnet_confusion_matrix.png'\n",
        "if cm_path_stage2.exists():\n",
        "    print(f\"‚úì Confusion matrix found: {cm_path_stage2}\")\n",
        "    img = plt.imread(cm_path_stage2)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title('Stage 2 SegNet: Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"‚úó Confusion matrix not found: {cm_path_stage2}\")\n",
        "\n",
        "# Training curves\n",
        "curves_path_stage2 = VISUALIZATIONS_DIR / 'stage2_segnet_training_curves.png'\n",
        "if curves_path_stage2.exists():\n",
        "    print(f\"‚úì Training curves found: {curves_path_stage2}\")\n",
        "    img = plt.imread(curves_path_stage2)\n",
        "    plt.figure(figsize=(18, 5))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title('Stage 2 SegNet: Training Curves', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"‚úó Training curves not found: {curves_path_stage2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load v√† hi·ªÉn th·ªã segmentation predictions\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Visualizing Segmentation Predictions...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "vis_path_stage2 = VISUALIZATIONS_DIR / 'stage2_segnet_predictions.png'\n",
        "if vis_path_stage2.exists():\n",
        "    print(f\"‚úì Predictions visualization found: {vis_path_stage2}\")\n",
        "    img = plt.imread(vis_path_stage2)\n",
        "    plt.figure(figsize=(20, 15))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title('Stage 2 SegNet: Predictions on Test Set', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"‚úó Predictions visualization not found: {vis_path_stage2}\")\n",
        "    print(\"  C√≥ th·ªÉ script ch∆∞a ch·∫°y xong ho·∫∑c c√≥ l·ªói trong qu√° tr√¨nh training\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model v√† test predictions tr·ª±c ti·∫øp trong notebook\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Testing Trained SegNet Model...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "from config import DEVICE\n",
        "from src.models.segnet import SegNetWithPretrainedEncoder\n",
        "from src.data import create_bigearthnet_dataloaders, get_val_augmentation\n",
        "from src.utils import SegmentationMetrics\n",
        "\n",
        "# Load best model\n",
        "if best_model_stage2.exists():\n",
        "    print(f\"Loading best model from: {best_model_stage2}\")\n",
        "    \n",
        "    # Create model\n",
        "    model_trained = SegNetWithPretrainedEncoder(\n",
        "        encoder_name='resnet50',\n",
        "        num_classes=NUM_CLASSES,\n",
        "        encoder_pretrained=False\n",
        "    )\n",
        "    \n",
        "    # Load checkpoint\n",
        "    checkpoint = torch.load(best_model_stage2, map_location=DEVICE)\n",
        "    if 'model_state_dict' in checkpoint:\n",
        "        model_trained.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        model_trained.load_state_dict(checkpoint)\n",
        "    \n",
        "    model_trained = model_trained.to(DEVICE)\n",
        "    model_trained.eval()\n",
        "    print(\"‚úì Model loaded successfully!\")\n",
        "    \n",
        "    # Test on a few samples\n",
        "    val_transform = get_val_augmentation(120)\n",
        "    _, val_loader_test, _ = create_bigearthnet_dataloaders(\n",
        "        metadata_path=METADATA_PATH,\n",
        "        data_folders=BIGEARTHNET_FOLDERS,\n",
        "        reference_maps_folder=REFERENCE_MAPS_FOLDER,\n",
        "        corine_to_eurosat_mapping=CORINE_TO_EUROSAT,\n",
        "        train_transform=None,\n",
        "        val_transform=val_transform,\n",
        "        batch_size=4,\n",
        "        num_workers=0,\n",
        "        pin_memory=False,\n",
        "        num_classes=NUM_CLASSES\n",
        "    )\n",
        "    \n",
        "    # Get a batch\n",
        "    batch = next(iter(val_loader_test))\n",
        "    images = batch['image'].to(DEVICE)\n",
        "    masks = batch['mask']\n",
        "    \n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        predictions = model_trained(images)\n",
        "    \n",
        "    # Visualize\n",
        "    num_samples = min(4, len(images))\n",
        "    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n",
        "    fig.suptitle('Trained SegNet Predictions', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    for i in range(num_samples):\n",
        "        # Original image\n",
        "        img_np = denormalize_image(images[i].cpu())\n",
        "        axes[i, 0].imshow(img_np)\n",
        "        axes[i, 0].set_title('Original Image', fontweight='bold')\n",
        "        axes[i, 0].axis('off')\n",
        "        \n",
        "        # Ground truth mask\n",
        "        mask_np = mask_to_rgb(masks[i].numpy(), COLOR_PALETTE)\n",
        "        axes[i, 1].imshow(mask_np)\n",
        "        axes[i, 1].set_title('Ground Truth', fontweight='bold')\n",
        "        axes[i, 1].axis('off')\n",
        "        \n",
        "        # Prediction\n",
        "        pred_mask = torch.argmax(predictions[i], dim=0).cpu().numpy()\n",
        "        pred_mask_rgb = mask_to_rgb(pred_mask, COLOR_PALETTE)\n",
        "        axes[i, 2].imshow(pred_mask_rgb)\n",
        "        axes[i, 2].set_title('SegNet Prediction', fontweight='bold')\n",
        "        axes[i, 2].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\" Prediction visualization complete!\")\n",
        "else:\n",
        "    print(\" Model checkpoint not found. Please run Stage 2 training first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary\n",
        "\n",
        "### Training Summary:\n",
        "- **Stage 1**: Pre-trained encoder v·ªõi EuroSAT classification (s·ª≠ d·ª•ng `train_stage1.py`)\n",
        "- **Stage 2**: Trained SegNet v·ªõi BigEarthNet segmentation (s·ª≠ d·ª•ng `train_stage2_segnet.py` v·ªõi model t·ª´ `src/models/segnet.py`)\n",
        "\n",
        "### Results:\n",
        "- Checkpoints saved in `outputs/checkpoints/`\n",
        "  - Stage 1: `outputs/checkpoints/stage1/`\n",
        "  - Stage 2: `outputs/checkpoints/stage2_segnet/`\n",
        "- Visualizations saved in `outputs/visualizations/`\n",
        "- Logs saved in `outputs/logs/`\n",
        "\n",
        "### Model Architecture:\n",
        "- **SegNet**: S·ª≠ d·ª•ng `SegNetWithPretrainedEncoder` t·ª´ `src/models/segnet.py`\n",
        "- Encoder: Pre-trained t·ª´ Stage 1 (ResNet50)\n",
        "- Decoder: SegNet-style decoder v·ªõi transposed convolutions\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
